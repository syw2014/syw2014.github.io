<!DOCTYPE html>
<html>
  
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta name="author" content="悯生" />
  
  
  <title>Paper Notes:Attention is all you need | 踏路</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Paper Notes,Attention,Self-Attention,序列建模,序列模型并行化," />
  

  
  <meta name="description" content="该论文是Google在2017年的一个工作，单从论文题目来看能够让很多人神往，在做NLP任务时它没有提RNN，没有提CNN，而且可以获得state-of-the-art效果，最近就对这篇文章内容进行了细致分析，做笔记如下。">

  

  
    <script src="//cdn.jsdelivr.net/npm/leancloud-storage@3.11.1/dist/av-min.js" async></script>
  

  
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  

  
    <script src="//unpkg.com/valine/dist/Valine.min.js" async></script>
  

  

  <script>
  // theme-ad's config script
  // it can be used in every script
  
  window.AD_CONFIG = {
    leancloud: {"appid":"6sEpjxpdQ0zFbHII2mSfaKib-gzGzoHsz","appkey":"qdahfkeMY3jcS1ebdMnOQr7I","comment":true,"count":true},
    welcome: {"enable":true,"interval":10},
    start_time: "2016-02-10",
    passwords: ["efe07af7441da2b69c4a41e42e73be4db47f66010a56900788a458354a7373ec", ],
    is_post: true,
    lock: false,
    author: "悯生",
    share: {"twitter":true,"facebook":true,"weibo":true,"qq":true,"wechat":true},
    mathjax: true,
    page_type: "",
    root: "/"
  };
</script>

  <script src="/vendor/sha256.min.js"></script>
<script src="/js/auth.js"></script>
<script src="/js/index.js"></script>
<script src="/vendor/qrcode.min.js"></script>

  
    <link rel="icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/images/favicon.ico">
  

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" href="/css/index.css">
<link rel="stylesheet" href="/styles/components/highlight/highlight.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  
</head>
  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="site-header">
  <div class="site-header-brand">
    
      <span class="site-header-brand-title">
        <a href="/">踏路</a>
      </span>
    
    
      <span class="site-header-brand-motto"> | 静心思考认真生活</span>
    
  </div>
  <div class="site-header-right">
    <nav class="site-header-navigation">
      
        <a href="/" target="_self">首页</a>
      
        <a href="/archives/" target="_self">归档</a>
      
        <a href="/projects/" target="_self">项目</a>
      
        <a href="/tags/" target="_self">标签云</a>
      
        <a href="/categories/" target="_self">分类云</a>
      
        <a href="/friends/" target="_self">友链</a>
      
        <a href="/about/" target="_self">关于</a>
      
    </nav>
    <div class="site-header-btn">
      
        <a href="https://github.com/syw2014" target="_blank" id="site-github">
          <i class="fa fa-github-alt"></i>
        </a>
      
      <a href="javascript:void(0);" id="site-search">
        <i class="fa fa-search"></i>
      </a>
      <a href="javascript:void(0);" id="site-nav-btn">
        <i class="fa fa-ellipsis-v"></i>
      </a>
    </div>
  </div>
</header>
<nav class="table-content" id="site-nav">
  <div class="table-content-title">
    <span>导航</span>
  </div>
  <div class="table-content-main">
    <ol class="toc">
      
        <li class="toc-item">
          <a href="/" target="_self">
            首页
          </a>
        </li>
      
        <li class="toc-item">
          <a href="/archives/" target="_self">
            归档
          </a>
        </li>
      
        <li class="toc-item">
          <a href="/projects/" target="_self">
            项目
          </a>
        </li>
      
        <li class="toc-item">
          <a href="/tags/" target="_self">
            标签云
          </a>
        </li>
      
        <li class="toc-item">
          <a href="/categories/" target="_self">
            分类云
          </a>
        </li>
      
        <li class="toc-item">
          <a href="/friends/" target="_self">
            友链
          </a>
        </li>
      
        <li class="toc-item">
          <a href="/about/" target="_self">
            关于
          </a>
        </li>
      
    </ol>
  </div>
</nav>
<div id="site-process"></div>
    <main>
      
  <div class="passage">
  <div class="passage-meta">
    <span>
      <i class="fa fa-calendar"></i>2018-02-08
    </span>
    
      <span>
        | <a href="/categories/Paper-Notes/"><i class="fa fa-bookmark"></i>Paper Notes</a>
      </span>
    
    
      <span>
        | <i class="fa fa-unlock-alt"></i>UNLOCK
      </span>
    
  </div>
  <h1 class="passage-title">
    Paper Notes:Attention is all you need
  </h1>
  
  <article class="passage-article">
    <h2 id="序列模型架构"><a href="#序列模型架构" class="headerlink" title="序列模型架构"></a>序列模型架构</h2><p>对于一个序列分析问题通常采用<strong>encoder-decoder</strong>架构，通过encoder模块将一个输入$(x_1, x_2,…,x_n)$转换为另一个表示序列$(z_1, z_2,…,z_n)$,decoder模块在将这个表示转换为输出序列$(y_1,y_2,…,y_m)$，这是一个典型的Encoder-Decoder架构。这种架构在机器翻译、阅读理解、Caption等多个场景都有广泛的使用。</p>
<h2 id="序列建模"><a href="#序列建模" class="headerlink" title="序列建模"></a>序列建模</h2><p>什么是序列呢？在NLP中，一句话经分词之后就形成含有多个词的list，可以称作序列；一个用户在一个实际窗的动作，就形成一个动作序列；除此之外生活中很多数据都可以看成一个序列。那我们就需要面对一个问题，如何对序列进行建模。因为序列间有时序信息，很直观的会想到采用<strong>RNN</strong>，它能够学习到局部信息，<strong>缺点</strong>是，1）无法并行化，因为必须等待前一步处理之后才能处理当前步，这导致RNN网络在训练时需要很长时间，2）不能很好学习全局结构信息<br>第二个思路是Fackbook的工作利用<a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">CNN</a>，可以方便并行，且能够学的全局信息,最后一种方式就是Google<br>论文中提到<strong>Attention</strong>，只利用注意力机制实现序列建模，$y_t=f(x_t,W_1,W_2)$,$W_1$和$W_2$是另外一个序列句子，如果这三个序列是同一个序列的话，就称作是<strong>self-attention</strong>。</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><img src="http://upload-images.jianshu.io/upload_images/2423131-a7fbbac2f390012d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在这篇论文中，作者给出了具体的attention的形式，结构如上图。</p>
<script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中，Q,K,V矩阵大小分别是<script type="math/tex">n \times{d_k}, m \times{d_k},m \times{d_v}</script>，最终形成的结果是<script type="math/tex">n\times{d_v}</script>，即将<script type="math/tex">n\times{d_k}</script>的序列表示成<script type="math/tex">n\times{d_v}</script>的矩阵形式,<script type="math/tex">\sqrt{d_k}</script>是一个缩放因子，文中提到对于dot product在计算结果会很大，经过softmax之后会将元素值推向边界0或1，这而这些区域梯度信息很小不便于训练，所以采用这个数值进行scale，为什么是<script type="math/tex">\sqrt{d_k}</script>，假设<script type="math/tex">q</script>与<script type="math/tex">k</script>都是均值为0，方差为1的随机变量，那么他们的dot product，<script type="math/tex">q\circ{q}=\Sigma_{i=1}^{d_k}q_ik_i</script>,它是均值为0，方差为<script type="math/tex">d_k</script>的一个变量，因此这里采用<script type="math/tex">\sqrt{d_k}</script>。论文中提到，Q,K,V分别是query、key-values矩阵的矩阵形式，那么对于单个query<script type="math/tex">q_t</script>来说，Attention计算如下，</p>
<script type="math/tex; mode=display">Attention(q_t,K,V)=softmax(q_t,K,V)=\Sigma_{i=1}^{m}\frac{exp(q_t\circ{k_i})}{Z}v_i</script><p>其中Z是整理后的系数，首先计算出$q_t$与$v_i$的相关性，然后加权求和。对于attention的使用方式一般分为两种，一种是让<script type="math/tex">q_t</script>与<script type="math/tex">k_i</script>采用相加(additive attention)然后再通过其他运算比如tanh等，另一种就是上式中<script type="math/tex">q_t</script>与<script type="math/tex">k_i</script>直接相乘的形式(dot-product/multiplicative)。这attention形式论文中称作<strong>scale Dot-Product Attention</strong>。</p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>这个是Goolge提出的对attention机制的完善，从字面翻译是多头attention，架构图如下。<br><img src="http://upload-images.jianshu.io/upload_images/2423131-f9816e3d7c6fcc38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>multi-head attention思想很简单，就是首先对querie，keys，values做个线性映射，然后将上述attention机制重复h次，将最终结果进行concat，就得等到multi-head attention结果。这种思想类似于CNN，通过不同的卷积核来得到更多的feature map。</p>
<script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^{O} \\
head_i = Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})</script><p>其中，<script type="math/tex">W_{i}^{Q} \in R^{d_{model}\times{d_k}}, W_{i}^{K} \in R^{d_{model}\times{d_k}}, W_{i}^{V} \in R^{d_{model}\times{d_v}}, W^{O} \in R^{hd_v \times{d_{model}}}</script>,这些都是不同的参数矩阵，需要通过数据学习得到。这里推算下参数shape，</p>
<script type="math/tex; mode=display">Q_{shape}: n \times d_{model}, W_{shape}^{Q}: d_{model}\times{d_k}, QW_{shape}:n \times d_k \\
K_{shape}: n \times d_{model}, W_{shape}^{K}: d_{model}\times{d_k}, KW_{shape}:n \times d_k \\
V_{shape}: n \times d_{model}, W_{shape}^{V}: d_{model}\times{d_v}, VW_{shape}:n \times d_v \\
Head_i(QW^{Q}, KW^{K}, VW^{V})_{shape}=n \times d_v \\
concat(head_1,...,head_h)_{shape}:n \times hd_v    \\
W_{shape}^{O}: hd_v \times d_{model} \\
MultiHead_{shape}: n \times d_{model}</script><p>其中，n代表输入序列长度，$d<em>{model}$代表模型为维度，在论文中，提到$$d_k=d_v=d</em>{model}/h=64,h=8$$.<br>通过multihead方式不仅能够得到具有注意力结果的表示，同时也含有序列全局信息。</p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>了解了Attention机制之后，那就有个疑问，到底什么是query，key 和value呢？对于这三部分的设计需要根据具体的任务，比如阅读理解中，query可以是文章的词向量序列，而key=value是问题的词向量序列。在这篇文章中，Google提到更多的是self-attention，即$Attention(X,X,X)$，为什么需要self-attention（inner attention）,主要原因如下，</p>
<ul>
<li>每层计算复杂度降低，</li>
<li>计算并行化</li>
<li>长距离依赖<h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2>目前为止，利用attention机制可以得到一个好的表示，但是这个表示并不含有序列中元素的相对或绝对为止，比如对于一篇文章，调换句子为止，或句子中词的位置，会得到相同的表示，因此论文中又提出了position encoding，具体如下，<script type="math/tex; mode=display">PE(pos,2i)=sin(pos/10000^{2i/d_{moddel}}) \\
PE(pos,2i+1)=cos(pos/10000^{2i/d_{moddel}})</script>其中pos代表的是位置，i代表维度中i个元素值就是pe(i).</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说，Google提出了一种新的思路来完成序列建模任务，但多少还是有些借鉴CNN思想，attention机制逐渐成为各种任务标配，因此掌握attention机制的原理，学习如何根据不同任务设计attention尤为重要。</p>
  </article>
  <aside class="table-content" id="site-toc">
  <div class="table-content-title">
    <i class="fa fa-arrow-right fa-lg" id="site-toc-hide-btn"></i>
    <span>目录</span>
  </div>
  <div class="table-content-main">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#序列模型架构"><span class="toc-text">序列模型架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#序列建模"><span class="toc-text">序列建模</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention"><span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Head-Attention"><span class="toc-text">Multi-Head Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Attention"><span class="toc-text">Self-Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Position-Encoding"><span class="toc-text">Position Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li></ol>
  </div>
</aside>
  
    <aside class="passage-copyright">
      <div>本文作者: 悯生</div>
      
        <div>
          原文链接: 
          <a href="" target="_blank">http://syw2014.github.io/passages/2018-02-08-paper-notes-attention/</a>
        </div>
      
      <div>
        版权声明: 本博客所有文章除特别声明外, 均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议. 转载请注明出处!
      </div>
    </aside>
  
  
    <div class="passage-tags">
     
      <a href="/tags/Attention/"><i class="fa fa-tags"></i>Attention</a>
     
      <a href="/tags/Self-Attention/"><i class="fa fa-tags"></i>Self-Attention</a>
     
      <a href="/tags/序列建模/"><i class="fa fa-tags"></i>序列建模</a>
     
      <a href="/tags/序列模型并行化/"><i class="fa fa-tags"></i>序列模型并行化</a>
    
    </div>
  
</div>

    </main>
    
      
<div class="site-comment-contanier" data-plateform="leancloud">
  
    <p id="site-comment-info">
      <i class="fa fa-spinner fa-spin"></i> 评论加载中
    </p>
    <div id="site-comment"></div>
  
</div>
    
    <div class="site-footer-wrapper">
  <footer class="site-footer">
    
      
        <div class="site-footer-col">
          <h5 class="site-footer-title">博客推荐</h5>
          
            <span class="site-footer-item">
              <a href="https://www.mskitech.com/" target="_blank">踏路</a>
            </span>
          
            <span class="site-footer-item">
              <a href="https://www.jianshu.com/u/c44c1c14b248" target="_blank">简书</a>
            </span>
          
        </div>
      
    
    <div class="site-footer-info">
      <i class="fa fa-clock-o"></i> 本站已稳定运行<span id="site-time"></span>
    </div>
    
      <div class="site-footer-info">
        <i class="fa fa-paw"></i> 您是本站第30<span id="site-count"></span>位访客
      </div>
    
    
      <div class="site-footer-info">
        <i class="fa fa-at"></i> Email: jerryshi0110@gmail.com
      </div>
    
    <div class="site-footer-info">
      <i class="fa fa-copyright"></i> 
      2019 <a href="https://github.com/dongyuanxin/theme-ad/" target="_blank">Theme-AD</a>.
      Created by <a href="https://www.mskitech.com/" target="_blank">GodBMW</a>.
      All rights reserved.
    </div>
  </footer>
</div>
    <div id="site-layer" style="display:none;">
  <div class="site-layer-content">
    <div class="site-layer-header">
      <span class="site-layer-header-title" id="site-layer-title"></span>
      <i class="fa fa-close" id="site-layer-close"></i>
    </div>
    <div class="site-layer-body" id="site-layer-container">
      <div class="site-layer-input" id="site-layer-search" style="display: none;">
        <input type="text">
        <i class="fa fa-search"></i>
      </div>
      
        <div class="site-layer-reward" id="site-layer-reward" style="display: none;">
          
            <div>
              <img src="/images/wechat.png" alt="WeChat">
              
                <p>WeChat</p>
              
            </div>
          
            <div>
              <img src="/images/alipay.png" alt="AliPay">
              
                <p>AliPay</p>
              
            </div>
          
        </div>
      
      <div id="site-layer-welcome" style="display:none;"></div>
    </div>
  </div>
</div>
    

<div class="bottom-bar">
  <div class="bottom-bar-left">
    <a href="/passages/2018-03-03-text-semantics/" data-enable="true">
      <i class="fa fa-arrow-left"></i>
    </a>
    <a href="/passages/2018-01-30-cs244n-note4/" data-enable="true">
      <i class="fa fa-arrow-right"></i>
    </a>
  </div>
  <div class="bottom-bar-right">
    <a href="javascript:void(0);" data-enable="true" id="site-toc-show-btn">
      <i class="fa fa-bars"></i>
    </a>
    
      <a href="#site-comment" data-enable="true">
        <i class="fa fa-commenting"></i>
      </a>
    
    <a href="javascript:void(0);" id="site-toggle-share-btn">
      <i class="fa fa-share-alt"></i>
    </a>
    
      <a href="javascript:void(0);" id="site-reward">
        <i class="fa fa-thumbs-up"></i>
      </a>
    
    <a href="javascript:void(0);" id="back-top-btn">
      <i class="fa fa-chevron-up"></i>
    </a>
  </div>
</div>
    <div id="share-btn">
  
    <a id="share-btn-twitter" href="javascript:void(0);" target="_blank">
      <i class="fa fa-twitter"></i>
    </a>
  
  
    <a id="share-btn-facebook" href="javascript:void(0);" target="_blank">
      <i class="fa fa-facebook"></i>
    </a>
  
  
    <a id="share-btn-weibo" href="javascript:void(0);" target="_blank">
      <i class="fa fa-weibo"></i>
    </a>
  
  
    <a id="share-btn-qq" href="javascript:void(0);" target="_blank">
      <i class="fa fa-qq"></i>
    </a>
  
  
    <a id="share-btn-wechat" href="javascript:void(0);" target="_blank">
      <i class="fa fa-wechat"></i>
    </a>
  
</div>
    


  <script async>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




    
  </body>
</html>